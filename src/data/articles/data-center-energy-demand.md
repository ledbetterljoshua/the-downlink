## The Numbers

A Precedence Research report published February 10 lays out the trajectory: global data center electricity consumption sits at roughly 460 TWh today, is projected to hit 945 TWh by 2030, and could reach 1,300 TWh by 2035. For context, Japan — the world's fifth-largest economy — consumes about 900 TWh annually. By 2030, data centers alone would exceed that.

The IEA's own modeling is consistent. It estimates data centers will account for over 20% of electricity demand growth in advanced economies this decade, with AI workloads driving the acceleration.

## Why the Range Is So Wide

Precedence's sensitivity analysis is the most useful part of the report. They model three scenarios:

- **Lift-Off Case** (1,700 TWh by 2035): AI adoption accelerates beyond current trends. Data centers consume 4.4% of global electricity.
- **Base Case** (1,300 TWh): Current trajectory continues. 3.4% of global electricity.
- **Headwinds Case** (700 TWh): Slower AI adoption, strong efficiency gains. Under 2% of global electricity.

The difference between the high and low scenarios — a full 1,000 TWh — is roughly the entire electricity consumption of Germany and France combined. What determines where we land: how compute-intensive future AI models are, how quickly hardware efficiency improves, and whether the buildout meets physical constraints (grid connections, cooling water, skilled labor).

## What Companies Are Actually Doing

The corporate response has been to lock in power supply years ahead, often through unconventional channels:

- **Meta** signed deals with TerraPower, Oklo, and Vistra supporting up to 6.6 GW of nuclear capacity by 2035
- **Microsoft** contracted with Constellation Energy to restart Three Mile Island's nuclear plant by 2027
- **Google** signed agreements for power from small modular reactors
- **Caterpillar** struck a 2 GW deal to supply backup and bridge power to data centers

These aren't speculative bets. They're binding commitments made because grid capacity in key markets is already constrained.

## The Tension

There's an unresolved tension between two AI infrastructure stories running simultaneously. The efficiency narrative says each generation of hardware does more with less — Nvidia's Blackwell architecture is significantly more power-efficient per computation than Hopper. The demand narrative says total consumption is growing faster than efficiency gains can offset, because usage is scaling exponentially.

Both are true. The question is which one wins.

## What to Watch

- **Grid interconnection queues**: The real bottleneck isn't building data centers — it's connecting them to the grid. Wait times in some US regions exceed 5 years.
- **Efficiency benchmarks**: Track PUE (power usage effectiveness) numbers from hyperscalers. If they're improving, it means the efficiency side is keeping up. If they're flat or worsening, demand is outrunning engineering.
- **Nuclear timelines**: Most nuclear commitments are for 2030+. If AI demand curves don't flatten, the 2026-2030 gap will be filled by natural gas — which creates its own set of problems.
